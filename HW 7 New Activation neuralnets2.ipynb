{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b65288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "\n",
    "def sigmoidderivative(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0,x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "vec_sigmoid = np.vectorize(sigmoid)\n",
    "vec_sigmoidderivative = np.vectorize(sigmoidderivative)\n",
    "vec_tanh = np.vectorize(tanh)\n",
    "vec_tanhderivative = np.vectorize(tanh_derivative)\n",
    "vec_relu = np.vectorize(relu)\n",
    "vec_reluderivative = np.vectorize(relu_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d42af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHot(n: np.uint8) -> np.ndarray:\n",
    "    arr = np.zeros(10, dtype=np.uint8)\n",
    "    arr[n] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4d98633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLayer:\n",
    "\n",
    "    def __init__(self, numinputs:int, numoutputs:int, activation=None):\n",
    "\n",
    "        self.numinputs = numinputs\n",
    "        self.numoutputs = numoutputs\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(self.numoutputs, self.numinputs + 1)\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "    \n",
    "        outputs = self.weights @ inputs # this is \\vec{h}\n",
    "\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                outputs = vec_sigmoid(outputs)\n",
    "\n",
    "            case \"Softmax\":\n",
    "                denom = 0\n",
    "                for i in range(len(outputs)):\n",
    "                    denom += math.exp[outputs[i]]\n",
    "                    outputs[i] = math.exp(outputs[i])\n",
    "                outputs = outputs/denom\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here, delete the pass\n",
    "                #Alejandro shall not pass\n",
    "                outputs = np.maximum(0, outputs)\n",
    "            \n",
    "            case \"Tanh\":\n",
    "                # Put code here, delete the pass\n",
    "                outputs = np.tanh(outputs)\n",
    "\n",
    "\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "        \n",
    "    def ComputeLocalGradient(self, inputs):\n",
    "        # z is output after activation\n",
    "        # h is output after linear layer\n",
    "        # w are weights\n",
    "        # Need to compute three things:\n",
    "        # dz/dh\n",
    "        # dh/dw\n",
    "        # dh/dx\n",
    "\n",
    "        inputs = np.append(inputs, np.array([1]))\n",
    "        outputs = self.weights @ inputs\n",
    "\n",
    "\n",
    "        # This part computes dzdh, and has cases for various activation functions\n",
    "        match self.activation:\n",
    "            case \"Sigmoid\":\n",
    "                dzdh = np.diag(vec_sigmoidderivative(outputs))\n",
    "            case \"Softmax\":\n",
    "                n = len(outputs)\n",
    "                dzdh = np.zeros((n, n))\n",
    "                denom = 0\n",
    "                for i in range(n):\n",
    "                    denom += math.exp(outputs[i])\n",
    "                \n",
    "                for i in range(n):\n",
    "                    for j in range(n):\n",
    "                        if i == j:\n",
    "                            dzdh[i][j] = (denom * math.exp(outputs[i]) - (math.exp(outputs[i])**2))/(denom**2)\n",
    "                        else:\n",
    "                            dzdh[i][j] = -(math.exp(outputs[j]))*(math.exp(outputs[j]))/(denom**2)\n",
    "\n",
    "            case \"ReLU\":\n",
    "                # Put code here and remove pass\n",
    "                dzdh = np.diag(np.where(outputs > 0, 1, 0))\n",
    "            case \"Tanh\":\n",
    "                # Put code here and remove pass\n",
    "                dzdh = np.diag(1 - np.tanh(outputs**2))\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # This part computes dhdw        \n",
    "        dhdw = np.zeros((self.numoutputs, self.numoutputs, self.numinputs+1)) #because of bias\n",
    "        for i in range(self.numoutputs):\n",
    "            for j in range(self.numinputs):\n",
    "                dhdw[i,i,j] = inputs[i]\n",
    "            dhdw[i,i,self.numinputs] = 1\n",
    "            \n",
    "        # This part computes dhdx\n",
    "        dhdx = self.weights[:, :-1]\n",
    "\n",
    "\n",
    "        return (dzdh, dhdw, dhdx)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35480923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(3, 3, 6)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "Layer1 = NeuralLayer(5,3,\"Sigmoid\")\n",
    "#Layer1.weights = np.array([[1, 2, -1], [3, -2, 1]])\n",
    "test1 = np.array([1, 2,3,4,5])\n",
    "(dzdh, dhdw, dhdx) = Layer1.ComputeLocalGradient(test1)\n",
    "print(dzdh.shape)\n",
    "print(dhdw.shape)\n",
    "print(dhdx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a744c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, errorfunc=None):\n",
    "        \n",
    "        self.errorfunc = errorfunc\n",
    "        self.layers = []\n",
    "        self.numlayers = 0\n",
    "\n",
    "    def AppendLayer(self, layer: NeuralLayer):\n",
    "        # need to check that the new layer to be appended has same \n",
    "        # number of inputs as the last layer already in the network\n",
    "        if len(self.layers) > 0:\n",
    "            if layer.numinputs == self.layers[-1].numoutputs:\n",
    "                self.layers.append(layer)\n",
    "                self.numlayers += 1\n",
    "            else:\n",
    "                print(\"Error: number of inputs does not match previous layer\")\n",
    "        else:\n",
    "            self.layers.append(layer)\n",
    "            self.numlayers += 1\n",
    "\n",
    "        \n",
    "    def Evaluate(self, inputs):\n",
    "\n",
    "        outputs = []\n",
    "        outputs.append(self.layers[0].Evaluate(inputs))\n",
    "\n",
    "        for i in range(1,self.numlayers):\n",
    "            outputs.append(self.layers[i].Evaluate(outputs[i-1]))\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def ComputeError(self, inputs, trueoutputs):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        \n",
    "        if self.errorfunc == \"MSE\":\n",
    "            n = len(outputs[-1])\n",
    "            diffs = outputs[-1] - trueoutputs\n",
    "            err = np.dot(diffs, diffs)\n",
    "            err = err/(2*n)\n",
    "            return err\n",
    "\n",
    "    def BackPropagate(self, inputs, trueoutputs, learningrate):\n",
    "\n",
    "        outputs = self.Evaluate(inputs)\n",
    "        gradients = []\n",
    "\n",
    "        # Compute all the necessary gradients\n",
    "        for i in range(self.numlayers):\n",
    "            if i == 0:\n",
    "                tempinput = inputs\n",
    "            else:\n",
    "                tempinput = outputs[i-1]\n",
    "            \n",
    "            gradients.append(self.layers[i].ComputeLocalGradient(tempinput))\n",
    "\n",
    "        match self.errorfunc:\n",
    "            case \"MSE\":\n",
    "                dldz = (0.5) * (outputs[-1] - trueoutputs)\n",
    "\n",
    "            case \"CrossEntropy\":\n",
    "                dldz = np.zeros(len(trueoutputs))\n",
    "                spot = np.where(1 == trueoutputs)\n",
    "                dldz[spot] = 1/outputs[-1][spot]\n",
    "                \n",
    "            \n",
    "\n",
    "        # Update weights, working backwards\n",
    "\n",
    "        currgrad = dldz @ gradients[-1][0]\n",
    "    \n",
    "        for i in range(self.numlayers-1, -1, -1):\n",
    "            self.layers[i].weights -= learningrate * (currgrad @ gradients[i][1])\n",
    "            currgrad = currgrad @ gradients[i][0] @ gradients[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eef2ccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017855449612622438\n",
      "0.01664821266278719\n",
      "0.015682851637275928\n",
      "0.014889868303736915\n",
      "0.014223862366432296\n",
      "0.013654014058041754\n",
      "0.013158709817746451\n",
      "0.012722366655767883\n",
      "0.012333479296715041\n",
      "0.011983376570671994\n",
      "0.011665404838636413\n"
     ]
    }
   ],
   "source": [
    "MyNN = NeuralNetwork(errorfunc=\"MSE\")\n",
    "MyLayer1 = NeuralLayer(5, 3, \"Sigmoid\")\n",
    "MyLayer2 = NeuralLayer(3, 2, \"Sigmoid\")\n",
    "\n",
    "\n",
    "MyNN.AppendLayer(MyLayer1)\n",
    "MyNN.AppendLayer(MyLayer2)\n",
    "\n",
    "myinput = np.array([1,2,3,4,5])\n",
    "mytrue = np.array([1,0])\n",
    "\n",
    "print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "for i in range(10):\n",
    "    MyNN.BackPropagate(myinput, mytrue, 1)\n",
    "    print(MyNN.ComputeError(myinput, mytrue))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b2becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train length: 6000, shape of first element: (10,)\n",
      "x_train length: 6000, shape of first element: (784,)\n",
      "x_test length: 1000, shape of first element: (784,)\n",
      "y_test length: 1000, shape of first element: (10,)\n",
      "\n",
      "e.g. label: 5 -> Onehot: [0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Load data from MNIST database\n",
    "(x_train0, y_train0), (x_test0, y_test0) = tf.keras.datasets.mnist.load_data()\n",
    "assert x_train0.shape == (60000, 28, 28)\n",
    "assert x_test0.shape == (10000, 28, 28)\n",
    "assert y_train0.shape == (60000,)\n",
    "assert y_test0.shape == (10000,)\n",
    "\n",
    "# Prepare data for processing\n",
    "# x_train and x_test need to be reshaped and converted to np.float64\n",
    "# y_train and y_test need to be one-hot encoded\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for y in range(6000):\n",
    "    images = x_train0[y].reshape(28*28).astype(np.float64)\n",
    "    images = images/255.0\n",
    "    x_train.append(images)\n",
    "\n",
    "    y_train.append(OneHot(y_train0[y]))\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for x in range(1000):\n",
    "    images = x_test0[x].reshape(28*28).astype(np.float64)\n",
    "    images = j/255.0\n",
    "    x_test.append(images)\n",
    "\n",
    "    y_test.append(OneHot(y_test0[x]))\n",
    "\n",
    "print(f\"y_train length: {len(y_train)}, shape of first element: {y_train[0].shape}\")\n",
    "print(f\"x_train length: {len(x_train)}, shape of first element: {x_train[0].shape}\")\n",
    "print(f\"x_test length: {len(x_test)}, shape of first element: {x_test[0].shape}\")\n",
    "print(f\"y_test length: {len(y_test)}, shape of first element: {y_test[0].shape}\")\n",
    "print(f\"\\ne.g. label: {y_train0[0]} -> Onehot: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d12a2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train0[0].reshape(28*28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0adc12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12926952831670788\n",
      "0.11920614972365635\n",
      "0.10974446462647724\n",
      "0.10506988926572218\n",
      "0.10320285080023736\n",
      "0.10231082595498198\n",
      "0.1018052367530684\n",
      "0.10148367319967784\n",
      "0.10126233784610342\n",
      "0.101101090804367\n",
      "0.10097851476027368\n",
      "0.10088221299121865\n",
      "0.10080454180255786\n",
      "0.10074054461094754\n",
      "0.10068687258854643\n",
      "0.10064118471247582\n",
      "0.10060179675610281\n",
      "0.10056746689003619\n",
      "0.10053725983088709\n",
      "0.10051045799390532\n",
      "0.10048650176272197\n",
      "0.10046494834936104\n",
      "0.10044544284390486\n",
      "0.10042769744827844\n",
      "0.10041147632232779\n",
      "0.10039658435249928\n",
      "0.10038285870974355\n",
      "0.1003701624220108\n",
      "0.10035837942277122\n",
      "0.10034741069521969\n",
      "0.1003371712396858\n",
      "0.10032758766644675\n",
      "0.10031859626858541\n",
      "0.10031014146686176\n",
      "0.10030217454545545\n",
      "0.10029465261703521\n",
      "0.10028753777004248\n",
      "0.10028079636181837\n",
      "0.10027439842926182\n",
      "0.10026831719481985\n",
      "0.10026252865027259\n",
      "0.10025701120436992\n",
      "0.10025174538316\n",
      "0.10024671357402684\n",
      "0.10024189980616079\n",
      "0.10023728956154085\n",
      "0.10023286961158188\n",
      "0.1002286278754612\n",
      "0.10022455329683153\n",
      "0.10022063573618809\n",
      "0.10021686587661105\n",
      "0.10021323514097805\n",
      "0.10020973561904531\n",
      "0.10020636000304602\n",
      "0.10020310153066356\n",
      "0.10019995393440981\n",
      "0.10019691139657957\n",
      "0.10019396850907551\n",
      "0.10019112023749739\n",
      "0.10018836188897413\n",
      "0.100185689083289\n",
      "0.10018309772690988\n",
      "0.10018058398958718\n",
      "0.10017814428322662\n",
      "0.10017577524278173\n",
      "0.10017347370894317\n",
      "0.10017123671242951\n",
      "0.10016906145970876\n",
      "0.10016694531999955\n",
      "0.10016488581341947\n",
      "0.1001628806001641\n",
      "0.10016092747061181\n",
      "0.10015902433626403\n",
      "0.10015716922143858\n",
      "0.10015536025564338\n",
      "0.10015359566656692\n",
      "0.10015187377362651\n",
      "0.10015019298202413\n",
      "0.10014855177726249\n",
      "0.1001469487200806\n",
      "0.1001453824417718\n",
      "0.10014385163985004\n",
      "0.10014235507403513\n",
      "0.10014089156252906\n",
      "0.10013945997855964\n",
      "0.10013805924716848\n",
      "0.10013668834222333\n",
      "0.10013534628363767\n",
      "0.10013403213477882\n",
      "0.10013274500005183\n",
      "0.10013148402264385\n",
      "0.10013024838241735\n",
      "0.10012903729394043\n",
      "0.10012785000464394\n",
      "0.10012668579309611\n",
      "0.10012554396738556\n",
      "0.10012442386360518\n",
      "0.1001233248444295\n",
      "0.10012224629777836\n",
      "0.10012118763556166\n",
      "0.10012014829249882\n",
      "0.10012014829249882\n",
      "Final check evaluation: [array([2.05164944e-02, 9.99997123e-01, 2.90741419e-02, 9.99999487e-01,\n",
      "       2.31612417e-07, 9.99999986e-01, 1.63595975e-03, 2.45424953e-04,\n",
      "       1.97815358e-03, 3.37174125e-02])]\n"
     ]
    }
   ],
   "source": [
    "MyMNISTNetwork = NeuralNetwork(\"MSE\")\n",
    "MyMNISTNetwork.AppendLayer(NeuralLayer(28*28,10,\"Sigmoid\"))\n",
    "\n",
    "\n",
    "y_train0[0]\n",
    "\n",
    "#testinput = np.astype(x_train0[0].reshape(28*28), np.float64)\n",
    "testinput = x_train0[0].reshape(28*28)\n",
    "testinput = testinput.astype(np.float64)\n",
    "\n",
    "testinput /= 255.0\n",
    "#print(testinput.sum())\n",
    "#print(MyMNISTNetwork.Evaluate(testinput))\n",
    "#print(MyMNISTNetwork.layers[-1].weights.dtype)\n",
    "\n",
    "onehot = np.array([0,0,0,0,0,1,0,0,0,0])\n",
    "\n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "for i in range(100):\n",
    "    MyMNISTNetwork.BackPropagate(testinput, onehot, 10)\n",
    "    print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "    \n",
    "print(MyMNISTNetwork.ComputeError(testinput, onehot))\n",
    "\n",
    "print(\"Final check evaluation: \" + str(MyMNISTNetwork.Evaluate(testinput)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae961e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
